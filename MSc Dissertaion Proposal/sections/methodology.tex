\chapter{Experiment}
%\chapter{Methodology}

\section{Feature Extraction}
% what should I do? How to implement?
The ProsodyPro Praat script was ran to extract the prosodic features (listed in Fig) by calling the ParselMouth method parselmouth.praat.run\_file() with the "Interactive labeling" command. The inputs are .wav audio files and Praat style .TextGrid files.  

\subsection{Audio Files Processing}
The SwitchBoard corpus has a file type of .sph which needs to be converted to .wav. The telephone conversation nature of these audios make them two-channel wave files with each channel representing one speaker, while the transcripts are aligned to each individual speaker. By calling parselmouth.Sound().extract\_all\_channels(), each channel in the audio file was extracted and then can be saved as a mono-channel file.

To do prosodic feature extraction using ProsodyPro, not only audio files are required but also time-aligned word transcriptions. The TextGrid object in Praat can serve as such transcriptions. It has a multi-tier structure consisting of two kinds of tiers: an interval tier and a point tier. The former is a sequence of labelled intervals, with boundaries in between; the latter is a sequence of labelled points. The idea is to create an interval tier with the word duration as an  interval and the word itself as the label. All these could be done by calling Praat's "To TextGrid", "Insert boundary", and "Set interval text" methods using ParselMouth. With the .wav file and the .TextGrid file been created, ProsodyPro is able to implement feature extraction and store the data in a table as it is shown in (tab...)

When looping through the transcripts to create TextGrid files, some manipulation work should be noticed:
- the "Insert boundary" method cannot take 0 as input, should delete rows with time point 0 in the transcripts
- at one time point can only insert one boundary, need to delete rows with same time points in the transcripts

After TextGrid files being created, the author manually checked them to ensure that the codes did the right job.

\subsection{Extraction Prosodic Features}
The feature extraction process costs 3 to 5 minutes for each wave file. It will generate 17 files in the formats of spreadsheets or graphs for further statistical or graphical researches \citep{Xu2013}. Among these files, the .means file is the spreadsheet which stores the extracted prosodic data from the corresponding audio file. Using Python library Pandas, .mean files were concatenated into a single Pandas DataFrame as the data source. Here is an example showing the first ten rows of the DataFrame for word importance related experiments.

pic: features example  rowLabel word, the order corresponds to the texts in the transcripts 

\section{Data Cleaning}
Take a look at (table), the reader will have an idea of how noisy the data is. On the RowLabel column, there are a number of words marked with special characters such as "[]", "{}", and "-". Three examples are "[silence]", "{jazzercise}" and "-[becau]se". Apart from the special characters, there are other subtle issues like no values, mixed data types, etc, which can be shown by calling pandas.DataFrame.describe() function(table).

(Table: prosodic features)

(table: data summary)

\subsection{Special Characters}
Words with special characters account for 26.1\% of the word importance corpus and 1.39\% of the total SwithBoard corpus. With these special words being removed, the vocabulary size of the SwitchBoard corpus will drop from around 38,649 to 29,955. Special characters indicate different meanings: the square brackets relates to silence, noise, laughter and other sounds which are not so related to the research. Braces relates to noncanonical words--words that are not following the rules. The dash indicates that the word is unfinished with hesitation, repetition, self-correction and so forth which, depending on the way it is dealt with, will affect the result of the research. 

(Table: examples of spe-chars)

This research focuses on the relation between words and their speech features, therefore all the non-canonical  or unfinished terms, those with square brackets, were removed. Table (table) shows the difference before and after data cleaning.

(table: before-after cleaning)

%May cause missing values that hazard the computation of propability.

\subsection{Missing Values and Data Type }
 For some feature types，ProsodyPro failed to compute a numeric value, resulting in a "--undefined--" label. The majority of these '' undefined" labels come form the mean f0 value of speech signals of "[slilence]". A way to deal with these missing values is to label them as Not A Number (numpy.nan). Many algorithms in statistics and scientific computing require the data in the same type. Pandas.DataFrame.astype() method was used to cast the mixed data to numpy.float type.


regex

- stop words
nltk.download('stopwords')
from nltk.corpus import stopwords

appendix
['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"]

* inclimation = ['uh', 'oh', 'wow', 'well', 'yeah', 'um', 'uh-huh', 'okay', 'gee', 'ooh', 'um-hum']
intensity


（Generate descriptive statistics that summarize the central tendency, dispersion and shape of a dataset's distribution, excluding NaN values.
Analyzes both numeric and object series, as well as DataFrame column sets of mixed data types. The output will vary depending on what is provided.

Finding:
most frequent word: [silence], not relavant to the research;
NaN value in col maxf0, minf0 ..., maybe there exists string values like "--undefined--" or ones that are not float64;
Checking the .csv file, not only [silence], but [laughter], [noise], etc;）





\section{Data Summary?}
Before any further statistical analysis, it is very helpful to have a look at the central tendency, dispersion and shape of a dataset's distribution. Table() is a summary of the prosodic data of the word importance corpus produced by pandas.DataFrame.describe() method.

(table: wimp summary)

Histograms helps to understand the distribution of the numeric value that is not easy to see with the mean and median. With Python library Matplotlib, the histograms were easily drawn (fig). Normally, a scatter plot will reveal the general picture of the relationship between two variables whereas the discrete word importance data in this research make the scatter plot less informative as it is shown in fig().

(fig: histogram of select features)

(fig: scatter plot)

According to fig (), some features are normally distributed such as  mean intensity; Some are skewed, for instance, mean f0 and duration. The skewed ones seem to be more correlated to the word importance given their shapes. Considering the normality and other assumptions that Pearson Correlaiton Coefficient has, it makes Spearman Rank Order Correlation a more suitable measurement for this research.

\section{Spearman Rank Order Correlation Coefficient}
pearson, spearman, kadel?
spearman is suitable
equation
numpy faster than pandas



\section{Ethical, Professional and Legal Issues}
%\lipsum  % Replace with your text
This project will be implement according to the BSC Code of Conduct with a commitment to the author's professional integrity and in the public interest.

\begin{itemize}
    
    \item The data to use in this project is from public database, e.g. Switchboard corpus, and the software to use is open-source, e.g. Praat. The author will have due regard for public interest including privacy, equality and  the legitimate rights of Third Parties.
    
    \item This project will be done within the author's professional competence without stealing, copying or reproducing the work of others.
    
    \item The author promise to carry out his professional responsibilities with due care and
    diligence in accordance with the Relevant Authority’s requirements and will not misrepresent any data or information.
    
    \item The author will commit to the duty of his profession and seek to improve professional standards.
\end{itemize}




