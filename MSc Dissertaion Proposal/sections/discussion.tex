\chapter{Experiment}

Table () summarise the correlations between prosodic features, word importance and word probability, any two of them as a pair. Significant correlation figures are coloured and the colour changes with the significance. 

\section{Prosodic Features - Word Importance Correlation}

In general, it could be concluded that the prominent prosodic features do correlate to the word importance, but the correlation is not very significant. The prominent prosodic features are extended duration, largely changed pitch and energy. According to table (), duration is positively correlated to word importance with a Spearman correlation coefficient valued 0.471, the greatest among others; excursion size correlation value, 0.251,  is comparatively great too, and the mean f0 follows with a value of 0.121. As for the raised voice, the correlation value for mean intensity is also commendable. The max f0 location feature is related to both duration and pitch, and the correlation, valued 0.231, is relatively strong too.

table(Prosodic Features - Word Importance Correlation)

Have we missed something? Yes, the first derivative of f0--max velocity and final velocity. They seem not very correlated. However, the derivative value can be positive or negative. It is the absolute value that matters. When using the absolute value to compute the correlation, it could be observed that the correlation value between max velocity and the word importance increased from 0.074 to a commendable level at 0.141 whereas the coefficient for final velocity didn't change prominently.

table(Velocity - Word Importance Correlation)

\section{Word Importance - Word Predictability Correlation}
As it is clearly shown in table(), given a token size of 25,000, word importance has a strong negetive correlation with word predictability with the Spearman correlation coefficient valued -0.700. It proves our hypothesis that the more importance the word in a sentence, the less predictable it is. 

table(Word Importance - Word Predictability Correlation)

\section{Prosodic Features - Word Predictability Correlation}
Experiments proved that there is correlation between prominent prosodic features and word predictability as it is shown in table(), and the correlation is negative. With one prominent prosodic feature, for instance mean intensity, increasing, the word predictability tends to decrease, hence the word is less predictable. With a token size of 25,000, the most correlated feature is duration with its correlation coefficient valued -0.565. Excursion size and max f0 location rank second with a value around -0.28. The figure for mean intensity, -0.116, is noticeable, too. The p-value (section ) for each aforementioned prosodic features is much smaller than the significance level 0.05, indicating that the correlation is statistically highly significant.

table(Prosodic Features - Word Predictability Correlation, 25,000)
table(Prosodic Features - Word Predictability Correlation, 2m)
table(Prosodic Features - Word Predictability Correlation, 4m)

\section{A Neural Word Importance Classifier}
Automatic word importance detection and labelling could be adopted to various real time scenarios such as summarisation, caption generation and speech recognition. In reality, word importance modelling systems are mainly based on texts as they contain rich semantic and syntactical cues. In the speech domain, such texts come from human-annotated transcripts or autimatic speech recognition (ASR) systems, which are by nature expensive to aqquire or noised by word errors.

 Accordding to the results of our correlation experiments, some prosodic features do positively correlated to word importance, but is it feasible to just use accoustic features to build a word importance labelling system? Is there any non-linear relationship between the variates that is hard to capture by Spearman Rank Order Correlation measurement?  A simple neural network based word importance classifier (hereinafter the "MLP-Softmax" model) was implemented to futher test the hypothesis and to explore how the prosodic features could contribute to the word importance labelling task.
 
 \subsection{Dataset}
 The RIT Word Importance Corpus (section   ) and its corespondding SwithBoard audio files were utilised as the dataset with 90\% for training and 10\% for testing. The size of the word tokens is 24,614. The word importance values were splited into three classes ranging [0 - 0.3), [0.3, 0.6), [0.6, 1] with a label Low Importance (L), Medium Importance (M) and High Importance (H) respectively. 
 
  \subsection{Model Structure}
  To test word-level prosodic features without using any context information, our model adopted a very simple structure: the input is a 11 dimensional vector with each dimension corresponding to one prosodic feature; there is only one hiden layer with Rectified Linear Unit (ReLU) as the activation function; a softmax layer was used to make prediction based on a normalised distribution over 3 labels.
 
 \subsection{Traning}
 For training, model parameters were initialised using Xavier initialisation. Negative Log Likelihood Loss was chosen as the loss function. We used the Adam optimizer with an initial learning rate of 0.001 to do full batch learning. After fine-tuning, the best-working setup was: a 32-dimensional hidden layer and a 0.05 learning rate. We used the accuracy result to evaluate the performance of the model. 
 
 table(Results)
 
 \subsection{Results and Analysis}
After 100 epochs of training, the MLP-Softmax model acchieved a accuracy score of 62.54. To evalutate the result, we compared it to the performances of another 4 models with the similar experiment setup(site ). (cite, 2019) also implemented a accoustic-prosodic based word importance classifier whereas the speech features they used were more complicated, in total 30 features related to pitch(10), energy (11), voicing (3) and spoken-lexical elements (6). It was a sequence-to-sequence model using LSTM to capture the context information.

As it is described in (cite 2019), althought the text-based models outperformed the speech-based models because of the significant semantic information available, their LSTM-Softmax model were competitive with an accuracy score of 63.72 especially in performance with text-based models noised by human-made word errors. According to (cite, Barker) it is typical for modern ASR systems to have a WER aound 30\% in many real-world scenarios. Thus, speech-based systems have huge potential.

Compared with the LSTM-Softmax model, our simple MLP-Softmax model only have 1/3 of the parameter numbers, yet has acchieved a very similar performance without using any context information. It could be inferred that, considering only the speech signals, the importance of a word is more related to the word itself. Additionally, the prominent prosodic features such as duration, intensity, largely changed pitches seem to be more important features to be selected.

\section{Further Work}
1. male female raw pitch
2. speaker specific or speaker normalised
3. word specific
4. context
5. general, not pragmatic.
	eg. whisper

regex

- stop words
nltk.download('stopwords')
from nltk.corpus import stopwords

appendix
['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', "you're", "you've", "you'll", "you'd", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', "she's", 'her', 'hers', 'herself', 'it', "it's", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', "don't", 'should', "should've", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', "aren't", 'couldn', "couldn't", 'didn', "didn't", 'doesn', "doesn't", 'hadn', "hadn't", 'hasn', "hasn't", 'haven', "haven't", 'isn', "isn't", 'ma', 'mightn', "mightn't", 'mustn', "mustn't", 'needn', "needn't", 'shan', "shan't", 'shouldn', "shouldn't", 'wasn', "wasn't", 'weren', "weren't", 'won', "won't", 'wouldn', "wouldn't"]

* inclimation = ['uh', 'oh', 'wow', 'well', 'yeah', 'um', 'uh-huh', 'okay', 'gee', 'ooh', 'um-hum']
intensity


（Generate descriptive statistics that summarize the central tendency, dispersion and shape of a dataset's distribution, excluding NaN values.
Analyzes both numeric and object series, as well as DataFrame column sets of mixed data types. The output will vary depending on what is provided.

Finding:
most frequent word: [silence], not relavant to the research;
NaN value in col maxf0, minf0 ..., maybe there exists string values like "--undefined--" or ones that are not float64;
Checking the .csv file, not only [silence], but [laughter], [noise], etc;）



