15/08/19
spearman correlation
    word importance has only 11 values, normalise the prodsodic features?
        - categorical or numerrical?
        - data binning: divide the data into intervals 

    - variavbles are ordinal
    - data are non-linearly related
    - data are non-normally distributed

chi square test for independence

14/08/19
supervisor meeting 10
1. write in your report how other people could reproduce your work
    data processing
2. care about data size
    what the performance if given less data? data is hard to get in some domain
3. the poster sessetion weights 10%
4. machine learning
    I got features and labels, why not machine learning?
    - features are independent?
    - feature combination
    - non-linear relationship
5. version control
    - code
    - outputs
    - never run a experiment twice
    - parameter-based easy experiment

Xu Hao's idea
1. traditional ML results as baseline
2. representation learning
    - feature value normalisation
    - data representation (raw data as vector)
    - representation learning based on selected features
        eg. similarity score

3. train a ML model
    - structure perceptron
    - naive bayes (probability)
    - deep learning
    * sk-learn library

4. evaluation
    - f1 score
    - ...
    
5. result
    compare to the baseline
    to prove that those prosodic features I selected could be captured by ML algorithms
6. Further work
    - different combination of features
    - context-related features
    - sentence level 





12/08/19
how many .wav files do I have?
45 with word importance annotations

supervisor meeting 9
- the project is assessed based on the poster sessetion? the codes? or the report? (didn't ask)
- spearman rank correlation
    Pearson cares about the two curves
    we care ranks
- the way of demostrating 



how long to process one?
10/08/19
anova test
t-test

feature selection
    labeled interval
1. continuous f0 velocity: (= first derivative of f0) curves (for labeled intervals only)
2. sampled f0: f0 at fixed time intervals as determined by F0_sample_rate (number of points per second)

X.means -- Containing the following values (in the order of the columns):

3. maxf0 (Hz)
4. maxf0_loc_ms -- Time of the f0 peak relative to the onset of an interval in milliseconds
5. maxf0_loc_ratio -- Relative location of the f0 peak as a proportion to the duration of the interval
    　# maybe useful at sentence level: locate the word with the highest f0.

6. minf0 (Hz)
7. meanf0 (Hz) average F0 in Hz
    # [silence] = undefined = 0
    # some words not [silence] = undefined ? eg. to, you, 
8. finalf0 (Hz) -- Indicator of target height (taken at a point specified by "Final offset" in the startup window); 
                   F0 near the interval offset in Hz
    # [silence] has value , not undefined; value around 120 Hz ? approximate to the previous value?
    # each value a bit lower than corresponding meanf0
9. excursion size (semitones) -- difference between maximum F0 in semitones and minimum F0
10. max_velocity (semitones/s)
11. final_velocity (semitones/s) -- Indicator of target slope (taken also at a point earlier than the interval offset by time specified by "Final offset" in the startup window)
12. mean intensity (dB)
13. duration (ms) -- interval duration in ms
    
    real time
1. rawf0 (Hz) -- raw f0 with real time computed directly from the pulse markings, converted directly from vocal periods (F0 = 1/T, where T is vocal period in second) marked in X.pulse
2. f0 (Hz) -- smoothed f0 with the trimming algorithm (Xu, 1999)
3. samplef0 (Hz) -- f0 values at fixed time intervals specified by "f0 sample rate"
4. smoothf0 (Hz) -- samplef0 f0 smoothed by a triangular window
5. semitonef0 —- Semitone version of X.samplef0 
6. f0velocity (semitones/s) -- velocity profile (instantaneous rates of F0 change) of f0 contour in semitone/s at fixed time intervals specified by "f0 sample rate" ***
    # semitone value ? around 80?
    
    time-normalized
1. timenormf0 (Hz) -- time-normalized f0. The f0 in each interval is divided into the same number of points (default = 10).
2. timenormIntensity (dB) -- time-normalized intensity. The intensity in each interval is divided into the same number of points (default = 10).
3. actutimenormf0 (Hz) -- time-normalized f0 with each interval divided into the same number of points (default = 10). But the time scale is the original, except that the onset time of interval 1 is set to 0, unless the "Set initial time to 0" box in the startup window is unchecked.
   actutimenormf0 — Time-normalized F0 (Hz) with real-time x-axis values 
4. normtime_f0velocity —Time-normalized continuous F0 velocity
   



focus -- F0





07/08/19
forced alignment
    data 
    - TextGrid: -s flag tier name? speaker name

    
06/08/19
supervisor meeting 8
do forced alignment
know more about Purdue features, derivatives = slopes?
report writing and result analysis in the same time
use diagrams and UML to demostrate workflow
project assessment: codes = reprot (I doubt it), the quality of it?
readings: writing for computer science

pblms:
1. missing phone.TextGrid to run the Purdue tool.

    buy phone annotations

    use forced alignment
        collections:
        https://github.com/pettarin/forced-alignment-tools

        Praatalign
        https://github.com/dopefishh/praatalign

        Montreal force aligner

            dictionary: prododylab-aligner
            https://github.com/prosodylab/Prosodylab-Aligner/blob/master/eng.dict

05/08/19

ask roger

resources:
1. The Switchboard Corpus in NXT
http://groups.inf.ed.ac.uk/switchboard/index.html

correlation
1. https://pypi.org/project/correlation-pearson/

prosody
read the same sentence in different ways:
    - like a robort
    - question or statement?
    - language learners learn the intonation

word level:
    - DON'T record this! (energy)
    - it's FAAAR from satisfied. (duration & pitch?) 
        ? how to judge a word which is spoken with longer duration than it usually is?
        ? can you get some info even if when you don't know the language?
            our aim: small batch --> big batch
        ? what role does pause / rhyme / phone play?
        ? do we need to go to phone level?
energy: words with more energy are more important?


04/08/19
ideas:
1. google word predicability
2. how pf are computed, prosody theories
pblms:
1. Actually, on spkr A' channel, spkr B's voice just got reduced, not disappear; whoese pitch contour is different from that of the two channels'.
2. Learn more stats 

gains:
1. write functions
2. realtive & absolute path

03/08/19
ask roger
1. opensource .TextGrid files
2. code uploading to the Department? assessment based on codes?
3. 

pblms:
1. Switchboard .wav files have two channels: spkr A and B -- needs two tiers on .TextGrid -- work on Purdue tool?

gains:
1. examine the data before coding.

02/08/19
Praat annotation
https://gist.github.com/scjs/ffbbba71cc8b3ff9d0476c82b2df9d0f

TGT
https://textgridtools.readthedocs.io/en/stable/api.html#tgt.core.Interval


01/07/19
paper:

tested prosodic feature extraction (failed)
ran ProsodyPro script, cannot find esambled files which record features such as f0_min, etc.

downloaded anotated word importance data from Rochester Institute.
url:

20/06/19
branch entropy


19/06/19
paper:
Automatic key term extraction from spoken course lectures using branching entropy and prosodic/semantic features

prosodic feature extraction tools:
- ProsodyPro * ---- A Praat script for large-scale systematic analysis of continuous prosodic events
- HTK forced alignment: segment the phonetic units 

It was argued that prosodic features in conference lectures are less useful because of the speaker variability.

Duration
Pitch: ESPS and smoothing
Energy: 0-th cepstral coef-ficient
maximum, minimum, mean and range


2019.06.14

intonational prominience?   
prosodic variation?

one meaning --> many ways:
Focus
- intonational prominience
- intonational phrasing variation
- word order variation
- syntactic constructions  
*various ways to convery meaning* --> finding likelihood

model the variability of a single speaker --> large labelled corpus

one prosodic feature --> many meanings:
- a single intonational contour
- a change of involvement
- a shift of topic
- a return from a perenthetical remark  
*how these functions interact?* -- barely studied! --> hard to compose/decompose prosodic features

### A framework of intonational  description: ToBI

http://www.speech.cs.cmu.edu/tobi/ToBI.1.html

text-to-speech
concept-to-speech?
speech dialogue system


pitch accent
phrase accent
boundary tone

prosodic grouping
intemidiate phrase

#### Four tiers of ToBI
- a tone tier
discribes intonation pattern
consists labels of pitch events: high (H), low (L) with diacritics
indicate intonational functions (pitch accent, phrasing)

- an orthographic tier
be used to interface the transcription to dictionary entries

- a break-index tier
marks the prosodic grouping of the words
numbers indicate the subjective strength of its association with the next word

- a miscellaneous tier
a `comment' tier to mark events such as the cough and disfluency

##### Break indices

    Level 4 phrase:
        Level 3 phrase(s)  
            pitch accent(s) -- stressed syllable of lexical items  
            phrase accent: high (H-) or low (L-)  
        boundary tone (H% or L%)

##### Contour variation
- ending in L- + H % -- continuation rise
- continuation rise + (L* + H) -- rise-fall-rise contour -- uncertainty or incredulity
- H* + H- + L% -- plateau contour -- bored or recitation
- H* + H- + H% -- high-rise question contour -- appreciation

##### 
pitch accent --> different tone targets + different `$F_0$` --> prominience  

***Pitch targets** are the smallest articulatorially operable units associated with linguistically functional pitch units such as tone and pitch accents.*
Prosody, Prominence and Perplexity in Spoken Language 

# Background Report Feedback
### Supervisor
** Student performance (supervisor only) **
Yuan has been very diligent in the run-up to to the start of his project and has met with me several times. 
He is very enthusiastic about the work, and should be able to produce a good outcome.
** Amount of work completed **
A reasonable amount of work has been done.
** Literature Survey or Background Information **
The background has been covered, but at a rather high level. 
The information provided may not be sufficient for a non-specialst to follow. 
More detail could have been provided.
** Analysis **
The analysis is rather shallow. 
Some key decisions appear to have been made without the pros and cons being explained (e.g. what are the alternatives to Switchboard?). 
The proposed experiments could have been described more clearly.
** Planning **
The plan looks OK, but there seems to be some experimental work in the theortical stage.
** Report Presentation **
The report is written reasonably well, although there are a number of typos. 
Many of the references are incomplete (e.g. missing the jounal name).
** General comments **
Yuan has started his project reasonably well, but he needs to step up his game if he wishes to achieve the marks to which he aspires.

### 2nd Marker
** General comments **
This is a clear report, albeit the shortest I have received. 
While the work is reasonably clear, I do not quite understand 
how the relationship model between prosodic features and importance features is built 
or what the actual outcome is. 
Further there is a wealth of literature on prosodic encoding that has not been described.


# Project Description

Background

One of the key functions of the ‘prosodic' patterning in speech is thought to be to draw attention to the key infomation-bearing elements of a spoken utterance.  In other words, the most important parts of an utterance are often associated with prosodic ‘prominence' such as increased loudness, large changes in the intonation, and extended segment durations.  From a linguistic perspective, the key information-bearing elements are those which are the least predictable.  Therefore, taking these two aspects together, it can be hypothesised that there should be a correlation between the predictability of words (known as ‘perplexity') and the major prosodic prominences in an utterance.

Project Goal

The aim of this 'Experimental' project is to test this hypothesis. Hence the project will involve the use of  (i) appropriate speech processing algorithms to derive prosodic features from a selected corpus of recorded utterances, (ii) appropriate text processing algorithms to compute the predictability of each individual word in the corresponding transcripts, and (iii) statistical tools to analyse the putative relationship between the two.  The prosodic features might be calculated using a toolkit such as ‘openSMILE’, and the perplexity might be calculated using the ‘CMU-SLM’ toolkit.



Reading

* Gold, B., & Morgan, N. (2000). Speech and Audio Signal Processing: Wiley.

* Hirschberg, J. (2002). Communication and prosody: Functional aspects of prosody. Speech Communication, 36(1–2), 31–43.

* The Munich Versatile and Fast Open-Source Audio Feature Extractor (openSMILE): http://audeering.com/technology/opensmile/

* The CMU-Cambridge Statistical Language Modeling Toolkit (CMU-SLM): http://www.speech.cs.cmu.edu/SLM/toolkit_documentation.html

### comments

prosody
- defination
- functions

information: perplexity, predicability,   
  
prosodic prominience  
- increased loudness: pitch, F0?, energy?  
- large intonation change  
- extended segment duration

correlation

words: prosodic features extracted by sentence? perplexity computed by sentence?
compute the predictabilty
- TF-IDF
- one word entropy


data
- spoken utterances? : need feature extraction; need transcripts
- ready annoted transcripts: may be ready to train LM
- ready trained LM with prosodic features? perfect!

prodsodic feature extraction  
- tools? openSMILE? Praat, ToBI, 
- coding myself

language model training
- ready LM
- tools
- coding myself

hypothesis evaluation
- tools
- method: prosodic value vs TF-IDF, or just compare the contour
